{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notre preprocessing comporte : la conversion de séquence d'ADN (string) en représentation 2D et l'ajout d'un padding pour remédier à la taille variable des séquences."
      ],
      "metadata": {
        "id": "-7zQPes5QCsG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYvscTx9PMjo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Fonctions conversions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Convertir les séquences en image\n",
        "def convert_to_pairings(seq):\n",
        "\n",
        "    # seq = seq[0]\n",
        "    result = []\n",
        "\n",
        "    AT_pair = []\n",
        "    TA_pair = []\n",
        "    CG_pair = []\n",
        "    GC_pair = []\n",
        "    unpaired = []\n",
        "\n",
        "    for i in range(len(seq)):\n",
        "        # base pairings\n",
        "        AT_pair_row = [0] * len(seq)\n",
        "        TA_pair_row = [0] * len(seq)\n",
        "        CG_pair_row = [0] * len(seq)\n",
        "        GC_pair_row = [0] * len(seq)\n",
        "        unpaired_row = [1] * len(seq)\n",
        "\n",
        "        for j in range(len(seq)):\n",
        "            if (seq[i] == 'A' and seq[j] == 'T'):\n",
        "                AT_pair_row[j] = 1\n",
        "                unpaired_row[j] = 1\n",
        "            elif (seq[i] == 'T' and seq[j] == 'A'):\n",
        "                TA_pair_row[j] = 1\n",
        "                unpaired_row[j] = 1\n",
        "            elif (seq[i] == 'C' and seq[j] == 'G'):\n",
        "                CG_pair_row[j] = 1\n",
        "                unpaired_row[j] = 1\n",
        "            elif (seq[i] == 'G' and seq[j] == 'C'):\n",
        "                GC_pair_row[j] = 1\n",
        "                unpaired_row[j] = 0\n",
        "\n",
        "        AT_pair.append(AT_pair_row)\n",
        "        TA_pair.append(TA_pair_row)\n",
        "        CG_pair.append(CG_pair_row)\n",
        "        GC_pair.append(GC_pair_row)\n",
        "        unpaired.append(unpaired_row)\n",
        "\n",
        "    result.append(AT_pair)\n",
        "    result.append(TA_pair)\n",
        "    result.append(CG_pair)\n",
        "    result.append(GC_pair)\n",
        "    result.append(unpaired)\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "# Appliquer cette conversion à tout un dataset\n",
        "def convert_to_pairings_dataset(dataset):\n",
        "    result = []\n",
        "    for seq in dataset:\n",
        "        result.append(convert_to_pairings(seq))\n",
        "    return result\n",
        "\n",
        "# Convertir les strings en image 2D\n",
        "def structure_to_matrix(structure):\n",
        "    length = len(structure)\n",
        "    matrix = np.zeros((length, length))\n",
        "    stack = []\n",
        "\n",
        "    for i, symbol in enumerate(structure):\n",
        "        if symbol == '(':\n",
        "            stack.append(i)\n",
        "        elif symbol == ')':\n",
        "            j = stack.pop()\n",
        "            matrix[i][j] = 1\n",
        "            matrix[j][i] = 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "# Ajoute un padding car séquences de taille variable\n",
        "def pad_structure(matrix, common_size, num_channels=None):\n",
        "    if num_channels:\n",
        "        padded_matrix = np.zeros((num_channels, *common_size))\n",
        "        for i in range(num_channels):\n",
        "            padded_matrix[i, :matrix.shape[1], :matrix.shape[2]] = matrix[i]\n",
        "    else:\n",
        "        padded_matrix = np.zeros(common_size)\n",
        "        padded_matrix[:matrix.shape[0], :matrix.shape[1]] = matrix\n",
        "    return padded_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La conversion des séquences en image + padding est couteux pour la mémoire vive, il faut donc répartir nos images en batch pour qu'elles soient chargées/déchargées en mémoire au besoin, sans surcharger notre RAM."
      ],
      "metadata": {
        "id": "3m12Lo3iQBoJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSUmlXVI_C0S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Preprocessing - Batching the data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load données\n",
        "def load_data(file_path):\n",
        "    return pd.read_pickle(file_path)\n",
        "\n",
        "# Conversion des données + padding\n",
        "def preprocess_and_convert(batch, common_size=(60, 60)):\n",
        "\n",
        "    # Convert sequences to pairings\n",
        "    # dataset_X = [convert_to_pairings(seq[0]) for seq in batch]\n",
        "    # dataset_Y = [seq[1] for seq in batch]  # Assuming second element is the label\n",
        "    # structures = [structure_to_matrix(seq[2]) for seq in batch]  # Assuming third element is the structure\n",
        "\n",
        "    dataset_X = [convert_to_pairings(seq) for seq in batch[0]]\n",
        "    dataset_Y = [seq for seq in batch[1]]  # Assuming second element is the label\n",
        "    structures = [structure_to_matrix(seq) for seq in batch[2]]  # Assuming third element is the structure\n",
        "    hairpins = [seq for seq in batch[3]]  # Assuming third element is the hairpins\n",
        "\n",
        "    # Pad sequences and structures\n",
        "    X_padded = [pad_structure(x, common_size, num_channels=5) for x in dataset_X]\n",
        "    # Y_padded = [pad_structure(y, common_size) for y in structures]  # Pad structures if they are labels\n",
        "\n",
        "    # return X_padded, Y_padded  # Return padded features and labels\n",
        "    # return X_padded, dataset_Y  # Return padded features and labels\n",
        "    return X_padded, hairpins  # Return padded features and labels\n",
        "\n",
        "\n",
        "# Function to save features and labels to a file\n",
        "def save_batch(features, labels, batch_index, output_dir):\n",
        "    batch_file = os.path.join(output_dir, f'batch_{batch_index}.npz')\n",
        "    np.savez_compressed(batch_file, features=features, labels=labels)\n",
        "\n",
        "# Function to process, preprocess (pad), and save the entire dataset in batches\n",
        "def process_and_save_batches(file_path, batch_size=100000, output_dir='batches_hairpins', common_size=(60, 60)):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    if isinstance(data, torch.utils.data.dataset.Subset):\n",
        "\n",
        "        data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
        "        total_batches = len(data_loader)\n",
        "\n",
        "        for i, batch_data in enumerate(data_loader):\n",
        "\n",
        "            print(f'Starting Batch {i+1}/{total_batches}.')\n",
        "\n",
        "            X_padded, Y_padded = preprocess_and_convert(batch_data, common_size)\n",
        "            save_batch(X_padded, Y_padded, i, output_dir)\n",
        "            print(f'Batch {i+1}/{total_batches} processed and saved.')\n",
        "\n",
        "    elif isinstance(data, list):\n",
        "        total_batches = (len(data) // batch_size) + (1 if len(data) % batch_size != 0 else 0)\n",
        "        for i in range(total_batches):\n",
        "            start_index = i * batch_size\n",
        "            end_index = start_index + batch_size\n",
        "            batch_data = data[start_index:end_index]\n",
        "\n",
        "            X_padded, Y_padded = preprocess_and_convert(batch_data, common_size)\n",
        "            save_batch(X_padded, Y_padded, i, output_dir)\n",
        "            print(f'Batch {i+1}/{total_batches} processed and saved.')\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported data type for batching\")\n",
        "\n",
        "\n",
        "file_path = '/content/gdrive/My Drive/train_5M_struct.pkl'\n",
        "batch_size = 100000\n",
        "process_and_save_batches(file_path, batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les architectures des modèles utilisés sont différentes dépendamment du type de donnée à prédire. Dans le cas des MFE (Float) et hairpins (Int), on utilise SimpleCNN(). Pour les structures secondaires (Matrice 2D) on utilise StructureCNN()."
      ],
      "metadata": {
        "id": "pypqx0CHQb3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTkXl7mC_5rs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Architecture CNN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers with batch normalization\n",
        "        self.conv1 = nn.Conv2d(5, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        # self.bn4 = nn.BatchNorm2d(256)\n",
        "        # self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        # self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 15 * 15, 512)\n",
        "        # self.fc1 = nn.Linear(512 * 15 * 15, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = F.relu(self.conv1(x))\n",
        "        # x = self.pool(x)\n",
        "        # x = F.relu(self.conv2(x))\n",
        "        # x = F.relu(self.conv3(x))\n",
        "        # x = self.adaptive_pool(x)  # Adaptive pooling layer\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = self.fc2(x)\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        # x = F.relu(self.bn4(self.conv4(x)))\n",
        "        # x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# class StructureCNN(nn.Module):\n",
        "#     def __init__(self, num_channels, output_size):\n",
        "#         super(StructureCNN, self).__init__()\n",
        "#         self.output_size = output_size\n",
        "#         self.conv1 = nn.Conv2d(num_channels, 16, kernel_size=3, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((self.output_size, self.output_size))\n",
        "#         self.fc1 = nn.Linear(64 * self.output_size * self.output_size, 512)\n",
        "#         self.fc2 = nn.Linear(512, self.output_size * self.output_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = self.adaptive_pool(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         x = x.view(-1, self.output_size, self.output_size)\n",
        "#         x = torch.sigmoid(x)\n",
        "#         return x\n",
        "\n",
        "class StructureCNN(nn.Module):\n",
        "    def __init__(self, num_channels, output_size):\n",
        "        super(StructureCNN, self).__init__()\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Convolutional layers with batch normalization\n",
        "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        # self.bn4 = nn.BatchNorm2d(256)\n",
        "        # self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        # self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully connected layers\n",
        "        reduced_size = output_size // 2**2  # Adjust based on the number of pooling layers\n",
        "        self.fc1 = nn.Linear(512 * reduced_size * reduced_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, output_size * output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        # x = F.relu(self.bn4(self.conv4(x)))\n",
        "        # x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Reshape to the output dimension and apply sigmoid activation\n",
        "        x = x.view(-1, self.output_size, self.output_size)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lors du training, on load les batches d'images nécessaires en mémoire.L'entrainement du model est lent, il nous fallait donc plusieurs sessions de training. Pour la prédiction des MFE (float) et hairpins (int) nous utilisons MSE (Mean Squared Error) comme loss. Pour la prédiction des structures secondaires, nous utilisons BCE (Binary Cross Entropy)."
      ],
      "metadata": {
        "id": "yPEnz7A4Q4rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ckVh-x_xz1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train model\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "# Function to load a batch from file, which includes features and labels\n",
        "def load_batch(file_path):\n",
        "    with np.load(file_path) as data:\n",
        "        return data['features'], data['labels']\n",
        "\n",
        "# Function to train the model using preprocessed batches from a directory\n",
        "def train_model_from_batches(batch_dir, model, criterion, optimizer, num_epochs=100, batch_size=128):\n",
        "    batch_files = [os.path.join(batch_dir, f) for f in os.listdir(batch_dir) if f.endswith('.npz')]\n",
        "    print(batch_files)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        sum_loss = 0\n",
        "        total_size = 0\n",
        "        avg_val_loss = 0\n",
        "\n",
        "        for batch_file in batch_files:\n",
        "\n",
        "            print(f'Loading {batch_file}')\n",
        "            features, labels = load_batch(batch_file)\n",
        "            batch_loss = 0\n",
        "\n",
        "            # Convert numpy arrays to tensors\n",
        "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "            labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "            # Split the batch into training and validation sets\n",
        "            X_train, X_val, y_train, y_val = train_test_split(features_tensor, labels_tensor, test_size=0.1, random_state=42)\n",
        "\n",
        "            # Create data loaders\n",
        "            train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "            # Training loop\n",
        "            model.train()\n",
        "            for inputs, labels in train_loader:\n",
        "\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                # if predicting mfe\n",
        "                outputs = outputs.squeeze()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                sum_loss += loss.item()\n",
        "                batch_loss += loss.item()\n",
        "\n",
        "            total_size += len(train_loader)\n",
        "\n",
        "            # Validation loop\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = outputs.squeeze()\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                val_loss /= len(val_loader)\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Batch processed, Batch Loss :{batch_loss / len(train_loader)} , Validation Loss: {val_loss:.6f}')\n",
        "            avg_val_loss += val_loss\n",
        "\n",
        "            # Free memory after processing each batch\n",
        "            del features, labels, features_tensor, labels_tensor, X_train, X_val, y_train, y_val, train_loader, val_loader\n",
        "            torch.cuda.empty_cache()  # If using CUDA\n",
        "\n",
        "        average_loss = sum_loss / total_size\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.6f}, Average val loss: {avg_val_loss / len(batch_files):.6f}', end='')\n",
        "\n",
        "        # torch.save(model.state_dict(), 'model_mfe.pth')\n",
        "        # !cp model_mfe.pth '/content/gdrive/My Drive/'\n",
        "\n",
        "\n",
        "\n",
        "batch_dir = 'batches_hairpins'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# model = StructureCNN(num_channels=5, output_size=60).to(device)\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# Continue the training\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/model.pth'))\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/model_mfe.pth'))\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/model_hairpins.pth'))\n",
        "\n",
        "# criterion = torch.nn.BCELoss()\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "train_model_from_batches(batch_dir, model, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois le modèle entrainé, on le test sur l'ensemble de test. Pour le rapport, il était important de regarder la performance du modèle sur des tailles de séquences différentes. On sépare donc les données en fonction de la longueur de la séquence. On applique ensuite le preprocessing sur ces données (Séquence vers image + padding et structure vers image + padding)."
      ],
      "metadata": {
        "id": "ydTD6heyRmH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and Preprocess test data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle('data_test.pkl')\n",
        "\n",
        "#Garder seulement les séquences de longueur <= 50\n",
        "data = [seq for seq in data if len(seq[0]) <= 50]\n",
        "\n",
        "#Séparer les séquences par longueur (de 10 a 20)\n",
        "data_10_20 = [seq for seq in data if len(seq[0]) <= 20]\n",
        "data_20_30 = [seq for seq in data if len(seq[0]) <= 30 and len(seq[0]) > 20]\n",
        "data_30_40 = [seq for seq in data if len(seq[0]) <= 40 and len(seq[0]) > 30]\n",
        "data_40_50 = [seq for seq in data if len(seq[0]) <= 50 and len(seq[0]) > 40]\n",
        "\n",
        "# Features\n",
        "data_X = [convert_to_pairings(seq[0]) for seq in data]\n",
        "data_X = [pad_structure(matrix, (60, 60), 5) for matrix in data_X]\n",
        "\n",
        "data_10_20_X = [convert_to_pairings(seq[0]) for seq in data_10_20]\n",
        "data_10_20_X = [pad_structure(matrix, (60, 60), 5) for matrix in data_10_20_X]\n",
        "\n",
        "data_20_30_X = [convert_to_pairings(seq[0]) for seq in data_20_30]\n",
        "data_20_30_X = [pad_structure(matrix, (60, 60), 5) for matrix in data_20_30_X]\n",
        "\n",
        "data_30_40_X = [convert_to_pairings(seq[0]) for seq in data_30_40]\n",
        "data_30_40_X = [pad_structure(matrix, (60, 60), 5) for matrix in data_30_40_X]\n",
        "\n",
        "data_40_50_X = [convert_to_pairings(seq[0]) for seq in data_40_50]\n",
        "data_40_50_X = [pad_structure(matrix, (60, 60), 5) for matrix in data_40_50_X]\n",
        "\n",
        "# Labels\n",
        "data_Y_mfe = [seq[1] for seq in data]\n",
        "data_Y_hairpins = [seq[3] for seq in data]\n",
        "\n",
        "data_Y_struct = [structure_to_matrix(seq[2]) for seq in data]\n",
        "data_Y_struct = [pad_structure(matrix, (60, 60)) for matrix in data_Y_struct]\n",
        "\n",
        "data_10_20_Y_mfe = [seq[1] for seq in data_10_20]\n",
        "data_10_20_Y_hairpins = [seq[3] for seq in data_10_20]\n",
        "\n",
        "data_10_20_Y_struct = [structure_to_matrix(seq[2]) for seq in data_10_20]\n",
        "data_10_20_Y_struct = [pad_structure(matrix, (60, 60)) for matrix in data_10_20_Y_struct]\n",
        "\n",
        "data_20_30_Y_mfe = [seq[1] for seq in data_20_30]\n",
        "data_20_30_Y_hairpins = [seq[3] for seq in data_20_30]\n",
        "\n",
        "data_20_30_Y_struct = [structure_to_matrix(seq[2]) for seq in data_20_30]\n",
        "data_20_30_Y_struct = [pad_structure(matrix, (60, 60)) for matrix in data_20_30_Y_struct]\n",
        "\n",
        "data_30_40_Y_mfe = [seq[1] for seq in data_30_40]\n",
        "data_30_40_Y_hairpins = [seq[3] for seq in data_30_40]\n",
        "\n",
        "data_30_40_Y_struct = [structure_to_matrix(seq[2]) for seq in data_30_40]\n",
        "data_30_40_Y_struct = [pad_structure(matrix, (60, 60)) for matrix in data_30_40_Y_struct]\n",
        "\n",
        "data_40_50_Y_mfe = [seq[1] for seq in data_40_50]\n",
        "data_40_50_Y_hairpins = [seq[3] for seq in data_40_50]\n",
        "\n",
        "data_40_50_Y_struct = [structure_to_matrix(seq[2]) for seq in data_40_50]\n",
        "data_40_50_Y_struct = [pad_structure(matrix, (60, 60)) for matrix in data_40_50_Y_struct]"
      ],
      "metadata": {
        "id": "90PWiG5nMOET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On calcule ensuite les prédictions du modèle sur nos ensembles de tests séparés."
      ],
      "metadata": {
        "id": "zaUJ0cKtSM9t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56a3I2VytZWz",
        "outputId": "33516fd8-2db3-4411-bb19-ce85ede71f00",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Get model prediction\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "def matrix_to_structure(matrix):\n",
        "    structure = ''\n",
        "    for i in range(matrix.shape[0]):\n",
        "        if sum(matrix[i]) == 0:\n",
        "            structure += '.'\n",
        "        else:\n",
        "            for j in range(matrix.shape[1]):\n",
        "                if matrix[i, j] == 1:\n",
        "                    if i < j:\n",
        "                        structure += '('\n",
        "                    else:\n",
        "                        structure += ')'\n",
        "    return structure\n",
        "\n",
        "def evaluate_model(model, features,labels, batch_size=128, isStruct = False):\n",
        "\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    test_loader = DataLoader(TensorDataset(features_tensor, labels_tensor), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Forward pass and round the outputs\n",
        "            outputs = model(inputs).cpu().numpy()\n",
        "\n",
        "            if(isStruct):\n",
        "              outputs_rounded = np.round(outputs)\n",
        "\n",
        "              for output in outputs_rounded:\n",
        "                  predictions.append(matrix_to_structure(output))\n",
        "\n",
        "              for label in labels.cpu().numpy():\n",
        "                  actuals.append(matrix_to_structure(label))\n",
        "\n",
        "\n",
        "            else:\n",
        "              for output in outputs.squeeze():\n",
        "                  predictions.append(output)\n",
        "\n",
        "              for label in labels.cpu().numpy():\n",
        "                  actuals.append(label)\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "# Call the function to evaluate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "## Si on test les structures secondaires\n",
        "\n",
        "# model = StructureCNN(num_channels=5, output_size=60).to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/model.pth'))\n",
        "\n",
        "# pred_data,actual_data = evaluate_model(model, features = data_X, labels = data_Y_struct)\n",
        "# pred_data_10_20,actual_data_10_20 = evaluate_model(model, features = data_10_20_X, labels = data_10_20_Y_struct)\n",
        "# pred_data_20_30,actual_data_20_30 = evaluate_model(model, features = data_20_30_X, labels = data_20_30_Y_struct)\n",
        "# pred_data_30_40,actual_data_30_40 = evaluate_model(model, features = data_30_40_X, labels = data_30_40_Y_struct)\n",
        "# pred_data_40_50,actual_data_40_50 = evaluate_model(model, features = data_40_50_X, labels = data_40_50_Y_struct)\n",
        "\n",
        "# # Si on test MFE\n",
        "\n",
        "# model = SimpleCNN().to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/model_mfe.pth'))\n",
        "\n",
        "# pred_data,actual_data = evaluate_model(model, features = data_X, labels = data_Y_mfe)\n",
        "# pred_data_10_20,actual_data_10_20 = evaluate_model(model, features = data_10_20_X, labels = data_10_20_Y_mfe)\n",
        "# pred_data_20_30,actual_data_20_30 = evaluate_model(model, features = data_20_30_X, labels = data_20_30_Y_mfe)\n",
        "# pred_data_30_40,actual_data_30_40 = evaluate_model(model, features = data_30_40_X, labels = data_30_40_Y_mfe)\n",
        "# pred_data_40_50,actual_data_40_50 = evaluate_model(model, features = data_40_50_X, labels = data_40_50_Y_mfe)\n",
        "\n",
        "# Si on test les hairpins\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/model_hairpins.pth'))\n",
        "\n",
        "pred_data,actual_data = evaluate_model(model, features = data_X, labels = data_Y_hairpins)\n",
        "pred_data_10_20,actual_data_10_20 = evaluate_model(model, features = data_10_20_X, labels = data_10_20_Y_hairpins)\n",
        "pred_data_20_30,actual_data_20_30 = evaluate_model(model, features = data_20_30_X, labels = data_20_30_Y_hairpins)\n",
        "pred_data_30_40,actual_data_30_40 = evaluate_model(model, features = data_30_40_X, labels = data_30_40_Y_hairpins)\n",
        "pred_data_40_50,actual_data_40_50 = evaluate_model(model, features = data_40_50_X, labels = data_40_50_Y_hairpins)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour comparer les performances du modèle, nous calculons plusieurs métriques :\n",
        "\n",
        "Mean Squared Error, Mean Average Error, Root Mean Squared Error, Le score R2 et la précision (accuracy) du modèle par rapport aux vraies valeurs.\n",
        "\n",
        "R2 est calculé seulement pour MFE (car floats) et accuracy seulement pour hairpins et structures secondaires."
      ],
      "metadata": {
        "id": "kwXytRLISdB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get metrics\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Calcul MSE / MAE\n",
        "def calculate_metrics(predictions, actuals):\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    return mse, mae\n",
        "\n",
        "# Si on prédit hairpins (int) : arrondir prediction au int le plus proche\n",
        "\n",
        "pred_data = np.round(pred_data)\n",
        "pred_data_10_20 = np.round(pred_data_10_20)\n",
        "pred_data_20_30 = np.round(pred_data_20_30)\n",
        "pred_data_30_40 = np.round(pred_data_30_40)\n",
        "pred_data_40_50 = np.round(pred_data_40_50)\n",
        "\n",
        "# Calcule MSE / MAE pour chaque ensemble\n",
        "mse_data, mae_data = calculate_metrics(pred_data, actual_data)\n",
        "mse_data_10_20, mae_data_10_20 = calculate_metrics(pred_data_10_20, actual_data_10_20)\n",
        "mse_data_20_30, mae_data_20_30 = calculate_metrics(pred_data_20_30, actual_data_20_30)\n",
        "mse_data_30_40, mae_data_30_40 = calculate_metrics(pred_data_30_40, actual_data_30_40)\n",
        "mse_data_40_50, mae_data_40_50 = calculate_metrics(pred_data_40_50, actual_data_40_50)\n",
        "\n",
        "# Print MSE / MAE\n",
        "print(\"MSE for data:\", mse_data)\n",
        "print(\"MAE for data:\", mae_data)\n",
        "print(\"MSE for data_10_20:\", mse_data_10_20)\n",
        "print(\"MAE for data_10_20:\", mae_data_10_20)\n",
        "print(\"MSE for data_20_30:\", mse_data_20_30)\n",
        "print(\"MAE for data_20_30:\", mae_data_20_30)\n",
        "print(\"MSE for data_30_40:\", mse_data_30_40)\n",
        "print(\"MAE for data_30_40:\", mae_data_30_40)\n",
        "print(\"MSE for data_40_50:\", mse_data_40_50)\n",
        "print(\"MAE for data_40_50:\", mae_data_40_50)\n",
        "\n",
        "# Define function to calculate RMSE\n",
        "def calculate_rmse(predictions, actuals):\n",
        "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
        "    return rmse\n",
        "\n",
        "# Calculate RMSE for each dataset\n",
        "rmse_data = calculate_rmse(pred_data, actual_data)\n",
        "rmse_data_10_20 = calculate_rmse(pred_data_10_20, actual_data_10_20)\n",
        "rmse_data_20_30 = calculate_rmse(pred_data_20_30, actual_data_20_30)\n",
        "rmse_data_30_40 = calculate_rmse(pred_data_30_40, actual_data_30_40)\n",
        "rmse_data_40_50 = calculate_rmse(pred_data_40_50, actual_data_40_50)\n",
        "\n",
        "# Print the RMSE for each dataset\n",
        "print(\"RMSE for data:\", rmse_data)\n",
        "print(\"RMSE for data_10_20:\", rmse_data_10_20)\n",
        "print(\"RMSE for data_20_30:\", rmse_data_20_30)\n",
        "print(\"RMSE for data_30_40:\", rmse_data_30_40)\n",
        "print(\"RMSE for data_40_50:\", rmse_data_40_50)\n",
        "\n",
        "# Calcule R2\n",
        "r2_data = r2_score(actual_data, pred_data)\n",
        "r2_data_10_20 = r2_score(actual_data_10_20, pred_data_10_20)\n",
        "r2_data_20_30 = r2_score(actual_data_20_30, pred_data_20_30)\n",
        "r2_data_30_40 = r2_score(actual_data_30_40, pred_data_30_40)\n",
        "r2_data_40_50 = r2_score(actual_data_40_50, pred_data_40_50)\n",
        "\n",
        "# Print score R2\n",
        "print(\"R2 for data:\", r2_data)\n",
        "print(\"R2 for data_10_20:\", r2_data_10_20)\n",
        "print(\"R2 for data_20_30:\", r2_data_20_30)\n",
        "print(\"R2 for data_30_40:\", r2_data_30_40)\n",
        "print(\"R2 for data_40_50:\", r2_data_40_50)\n",
        "\n",
        "# Print la précision (si hairpins ou structure secondaire)\n",
        "print(accuracy_score(pred_data,actual_data))\n",
        "print(accuracy_score(pred_data_10_20,actual_data_10_20))\n",
        "print(accuracy_score(pred_data_20_30,actual_data_20_30))\n",
        "print(accuracy_score(pred_data_30_40,actual_data_30_40))\n",
        "print(accuracy_score(pred_data_40_50,actual_data_40_50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCLPEvGgc9GE",
        "outputId": "08d5860e-079a-4dd1-b814-5805d594be5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for data: 0.15960975\n",
            "MAE for data: 0.1529756\n",
            "MSE for data_10_20: 0.014545455\n",
            "MAE for data_10_20: 0.014545455\n",
            "MSE for data_20_30: 0.1004\n",
            "MAE for data_20_30: 0.1\n",
            "MSE for data_30_40: 0.212\n",
            "MAE for data_30_40: 0.2044\n",
            "MSE for data_40_50: 0.326\n",
            "MAE for data_40_50: 0.3068\n",
            "MAE percentage for data: 5.099186673760414\n",
            "MAE percentage for data_10_20: 1.4545454643666744\n",
            "MAE percentage for data_20_30: 5.000000074505806\n",
            "MAE percentage for data_30_40: 6.813333183526993\n",
            "MAE percentage for data_40_50: 10.226666927337646\n",
            "MAE percentage relative to average for data: -11.527716368436813\n",
            "MAE percentage relative to average for data_10_20: -1.4311270788311958\n",
            "MAE percentage relative to average for data_20_30: -8.716876059770584\n",
            "MAE percentage relative to average for data_30_40: -14.465676248073578\n",
            "MAE percentage relative to average for data_40_50: -17.406105995178223\n",
            "RMSE for data: 0.3995119\n",
            "RMSE for data_10_20: 0.12060454\n",
            "RMSE for data_20_30: 0.3168596\n",
            "RMSE for data_30_40: 0.4604346\n",
            "RMSE for data_40_50: 0.5709641\n",
            "R2 for data: 0.4626930062282113\n",
            "R2 for data_10_20: 0.09632365784499142\n",
            "R2 for data_20_30: 0.21027063388957246\n",
            "R2 for data_30_40: 0.27005037839970025\n",
            "R2 for data_40_50: 0.27076081238266725\n",
            "MAPE for data: 8.93089473247528\n",
            "MAPE for data_10_20: 0.7636363618075848\n",
            "MAPE for data_20_30: 5.673333629965782\n",
            "MAPE for data_30_40: 12.64333426952362\n",
            "MAPE for data_40_50: 17.46000051498413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si on calcule R2, on peut faire un graphe pour avoir une représentation des prédictions de notre modèle."
      ],
      "metadata": {
        "id": "FkYcgJRZTHoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot R2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Calculate R2 score\n",
        "r2 = r2_score(actual_data, pred_data)\n",
        "\n",
        "# Plot the actual vs. predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(actual_data, pred_data, color='blue', label='Actual vs Predicted')\n",
        "plt.plot([min(actual_data), max(actual_data)], [min(actual_data), max(actual_data)], linestyle='--', color='red', label='Perfect Prediction')\n",
        "plt.xlabel('Vraies valeurs')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title(f'Predictions vs Labels (R2={r2:.2f})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jJiFd6nag815",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si on prédit la structure secondaire, il est pertinent d'avoir plus de métriques de performance, telle que le score F1"
      ],
      "metadata": {
        "id": "dX2haLQ6UNMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title More metrics if Secondary Structures\n",
        "from sklearn.metrics import accuracy_score,precision_score\n",
        "\n",
        "def compute_metrics(pred_data, actual_data):\n",
        "    # Convert concatenated sequences back to lists of characters\n",
        "    pred_data = list(pred_data)\n",
        "    actual_data = list(actual_data)\n",
        "\n",
        "    precision = precision_score(actual_data, pred_data, average=None, labels=[\".\", \"(\", \")\"])\n",
        "    recall = recall_score(actual_data, pred_data, average=None, labels=[\".\", \"(\", \")\"])\n",
        "    f1 = f1_score(actual_data, pred_data, average=None, labels=[\".\", \"(\", \")\"])\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Compute metrics for each fixed dataset and print the results\n",
        "print(\"Metrics for pred_data:\")\n",
        "precision, recall, f1 = compute_metrics(''.join(pred_data), ''.join(actual_data))\n",
        "print(\"Symbol '.', Precision:\", precision[0], \"Recall:\", recall[0], \"F1 Score:\", f1[0])\n",
        "print(\"Symbol '(', Precision:\", precision[1], \"Recall:\", recall[1], \"F1 Score:\", f1[1])\n",
        "print(\"Symbol ')', Precision:\", precision[2], \"Recall:\", recall[2], \"F1 Score:\", f1[2])\n",
        "\n",
        "# Repeat for other datasets\n",
        "print(\"\\nMetrics for pred_data_10_20:\")\n",
        "precision, recall, f1 = compute_metrics(''.join(pred_data_10_20), ''.join(actual_data_10_20))\n",
        "print(\"Symbol '.', Precision:\", precision[0], \"Recall:\", recall[0], \"F1 Score:\", f1[0])\n",
        "print(\"Symbol '(', Precision:\", precision[1], \"Recall:\", recall[1], \"F1 Score:\", f1[1])\n",
        "print(\"Symbol ')', Precision:\", precision[2], \"Recall:\", recall[2], \"F1 Score:\", f1[2])\n",
        "\n",
        "print(\"\\nMetrics for pred_data_20_30:\")\n",
        "precision, recall, f1 = compute_metrics(''.join(pred_data_20_30), ''.join(actual_data_20_30))\n",
        "print(\"Symbol '.', Precision:\", precision[0], \"Recall:\", recall[0], \"F1 Score:\", f1[0])\n",
        "print(\"Symbol '(', Precision:\", precision[1], \"Recall:\", recall[1], \"F1 Score:\", f1[1])\n",
        "print(\"Symbol ')', Precision:\", precision[2], \"Recall:\", recall[2], \"F1 Score:\", f1[2])\n",
        "\n",
        "print(\"\\nMetrics for pred_data_30_40:\")\n",
        "precision, recall, f1 = compute_metrics(''.join(pred_data_30_40), ''.join(actual_data_30_40))\n",
        "print(\"Symbol '.', Precision:\", precision[0], \"Recall:\", recall[0], \"F1 Score:\", f1[0])\n",
        "print(\"Symbol '(', Precision:\", precision[1], \"Recall:\", recall[1], \"F1 Score:\", f1[1])\n",
        "print(\"Symbol ')', Precision:\", precision[2], \"Recall:\", recall[2], \"F1 Score:\", f1[2])\n",
        "\n",
        "print(\"\\nMetrics for pred_data_40_50:\")\n",
        "precision, recall, f1 = compute_metrics(''.join(pred_data_40_50), ''.join(actual_data_40_50))\n",
        "print(\"Symbol '.', Precision:\", precision[0], \"Recall:\", recall[0], \"F1 Score:\", f1[0])\n",
        "print(\"Symbol '(', Precision:\", precision[1], \"Recall:\", recall[1], \"F1 Score:\", f1[1])\n",
        "print(\"Symbol ')', Precision:\", precision[2], \"Recall:\", recall[2], \"F1 Score:\", f1[2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTNI5GT0asAy",
        "outputId": "20a7d00e-90e4-4a24-e956-2b953d4d8552",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for pred_data: 0.5595121951219513\n",
            "Accuracy for pred_data_10_20: 0.8921818181818182\n",
            "Accuracy for pred_data_20_30: 0.6568\n",
            "Accuracy for pred_data_30_40: 0.438\n",
            "Accuracy for pred_data_40_50: 0.2178\n",
            "Metrics for pred_data:\n",
            "Symbol '.', Precision: 0.9587253950206435 Recall: 0.9824694679389165 F1 Score: 0.9704522165378114\n",
            "Symbol '(', Precision: 0.8840578015759815 Recall: 0.7690125088986067 F1 Score: 0.8225318713831962\n",
            "Symbol ')', Precision: 0.875219211073959 Recall: 0.7613241126817858 F1 Score: 0.814308401862246\n",
            "\n",
            "Metrics for pred_data_10_20:\n",
            "Symbol '.', Precision: 0.9961555601375485 Recall: 0.9961356834053874 F1 Score: 0.9961456216723146\n",
            "Symbol '(', Precision: 0.9613048522486862 Recall: 0.9615017064846416 F1 Score: 0.9614032692898338\n",
            "Symbol ')', Precision: 0.9561864464614754 Recall: 0.956382252559727 F1 Score: 0.9562843394874245\n",
            "\n",
            "Metrics for pred_data_20_30:\n",
            "Symbol '.', Precision: 0.9789157770770809 Recall: 0.9857915636931223 F1 Score: 0.9823416389642868\n",
            "Symbol '(', Precision: 0.8935001359804188 Recall: 0.8510076153965704 F1 Score: 0.871736361706644\n",
            "Symbol ')', Precision: 0.8856132716888768 Recall: 0.8434958296637828 F1 Score: 0.8640416047548292\n",
            "\n",
            "Metrics for pred_data_30_40:\n",
            "Symbol '.', Precision: 0.9508171360571201 Recall: 0.9765458856504983 F1 Score: 0.9635097817741851\n",
            "Symbol '(', Precision: 0.8682254697286013 Recall: 0.7624949580140076 F1 Score: 0.8119326057671657\n",
            "Symbol ')', Precision: 0.8587473903966597 Recall: 0.7541710975028418 F1 Score: 0.8030690537084398\n",
            "\n",
            "Metrics for pred_data_40_50:\n",
            "Symbol '.', Precision: 0.8987188565218107 Recall: 0.9668627207823268 F1 Score: 0.9315462474610409\n",
            "Symbol '(', Precision: 0.8516079310586422 Recall: 0.6551501104942596 F1 Score: 0.7405714982026442\n",
            "Symbol ')', Precision: 0.8407832971344497 Recall: 0.6468226162884708 F1 Score: 0.7311582282337171\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}